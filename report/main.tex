%------------------------------------------------------------------------------
% Beginning of journal.tex
%------------------------------------------------------------------------------
%
% AMS-LaTeX version 2 sample file for journals, based on amsart.cls.
%
%        ***     DO NOT USE THIS FILE AS A STARTER.      ***
%        ***  USE THE JOURNAL-SPECIFIC *.TEMPLATE FILE.  ***
%
% Replace amsart by the documentclass for the target journal, e.g., tran-l.
%
\documentclass{amsart}
\input{newmacros}
%\input{algorithm_labels.aux}
%     If your article includes graphics, uncomment this command.
\usepackage{graphicx}
\usepackage[backend=bibtex,style=numeric,natbib=true]{biblatex}
% Use the bibtex backend with the authoryear citation style (which resembles APA)
\usepackage{url}
\usepackage{citesort}
\usepackage{rotating}
\usepackage{geometry}
\usepackage{float}
\geometry{left=30mm, top=20mm}
\usepackage{hyperref}
\addbibresource{mainbib.bib} % The filename of the bibliography

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\numberwithin{equation}{section}


\newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}%
                     =}

\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}

%    Absolute value notation
\newcommand{\abs}[1]{\lvert#1\rvert}

%    Blank box placeholder for figures (to avoid requiring any
%    particular graphics capabilities for printing this document).
\newcommand{\blankbox}[2]{%
  \parbox{\columnwidth}{\centering
%    Set fboxsep to 0 so that the actual size of the box will match the
%    given measurements more closely.
    \setlength{\fboxsep}{0pt}%
    \fbox{\raisebox{0pt}[#2]{\hspace{#1}}}%
  }%
}

\begin{document}

\title{Probabilistic Algorithms for Finding \\ Matrix Decompositions}

%    Information for first author
\author{Alex Nowak}
%    Address of record for the research reported here
\address{Department of Mathematics \\
\'Ecole Normale Sup\'erieure de Cachan}
%    Current address
% \curraddr{Department of Mathematics and Statistics,
% Case Western Reserve University, Cleveland, Ohio 43403}
\email{alexnowakvila@gmail.com}
%    \thanks will become a 1st page footnote.
% \thanks{The first author was supported in part by NSF Grant \#000000.}

% %    Information for second author
% \author{Author Two}
% \address{Mathematical Research Section, School of Mathematical Sciences,
% Australian National University, Canberra ACT 2601, Australia}
% \email{two@maths.univ.edu.au}
% \thanks{Support information for the second author.}

%    General info
% \subjclass[2000]{Primary 54C40, 14E20; Secondary 46E25, 20C20}

% \date{January 1, 2001 and, in revised form, June 22, 2001.}

% \dedicatory{This paper is dedicated to our advisors.}

% \keywords{Differential geometry, algebraic geometry}

\begin{abstract}
Low-rank matrix approximations are obliquous in many areas ranging
from data analysis to scientific computing. From a data science point of
view, probably the most important application is due to Principal
Component Analysis (PCA), which aims to reveal hidden linear structure in
massive datasets through a low-rank matrix decomposition. 
Consequently, the complexity of the algorithm plays a central role in the
applicability of the algorithms to big data. The most common
approximative factorization
is the so-called truncated singular value decomposition (k-SVD) which can be 
computed in $\mathcal{O}(mnk)$ floating-point operations, where $k$ is the target rank
of the decomposition and $m$ and $n$ are the corresponding dimensions of the 
matrix.
In this review, we introduce to the reader randomized algorithms 
that can achieve the aforementioned task with numerous advantadges compared
to the classical algorithms. These algorithms are based on the fact that the
image of a low-rank matrix can be approximated by the action
of the matrix to a reasonable amount of random vectors from the input space.
Starting from this point, it is possible to develop
algorithms that achieve a complexity of $\mathcal{O}(mn\log{k})$ for 
dense-matrices, matches the flop count of classical
Krylov subspace methods for sparse matrices with a gain in robustness, and
for large matrices that can not be stored in memory (RAM), they
achieve a constant number of passes compared to the 
$\mathcal{O}(k)$ for classical algorithms.
\end{abstract}

\maketitle

\input{sections/introduction}
\input{sections/algorithms}
\input{sections/theory}
\input{sections/experiments}

\section*{Conclusion}
Randomization schemes for approximate matrix factorization are a powerful and 
versatile tool that \textbf{every data scientist should know about}.
These methods are more robust and scalable than classical algorithms,
and despite not achieving the same level of accuracy, the error gap can be
perfectly controlled.

Moreover, these methods rely on theoretical bounds resulting from theorems
on concentration of measure,
although we have seen in Experiment \ref{fig:exp1-1} that the bounds on the
expectation can be very loose for certain matrices. One interesting direction
of research can be the sharpening of bounds like \ref{thm:avg-frob-error-gauss}
under hypothesis on the matrix $\mtx{A}$.


There are several variants in this set of methods that
are designed for matrices with different properties. One drawback
is that sometimes they can be not that easy to verify, for e.g, the decaying
rate of the spectrum.

From a more personal point of view, I have personally appreciated both the 
theoretical and experimental section of this review. I think it is extremely 
important to have nice experimental results always grounded by theory.

\printbibliography[heading=bibintoc]
% \bibliographystyle{siam}

% \bibliographystyle{amsplain}
\end{document}

%------------------------------------------------------------------------------
% End of journal.tex
%------------------------------------------------------------------------------
\grid
